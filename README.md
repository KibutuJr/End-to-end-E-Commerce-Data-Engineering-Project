# ğŸ›’ End-to-End E-Commerce Data Pipeline  
### Built with Snowflake â€¢ dbt â€¢ Apache Airflow â€¢ SLA Alerting  

This project is a **real-world data engineering workflow**: an **E-Commerce Order Monitoring Pipeline** built with **Snowflake**, **dbt**, and **Apache Airflow**.  

The pipeline simulates raw order data, ingests it into **Snowflake**, applies **modular transformations with dbt**, orchestrates tasks using **Airflow**, and finally implements **SLA-driven email alerting for delayed orders**.  

This project reflects the type of **data pipelines used in production environments** for monitoring KPIs, ensuring data quality, and enabling business decision-making.

---

## ğŸ“Š Project Overview  

- **Data Simulation** â†’ Generate dummy order, customer, and product data  
- **Data Ingestion** â†’ Load into Snowflake raw schema  
- **Data Transformation** â†’ Clean, standardize, and model data with dbt  
- **Orchestration** â†’ Automate ingestion + transformation workflows with Airflow  
- **Monitoring & Alerting** â†’ SLA checks for delayed orders, with **email notifications**  

---

## ğŸ—ï¸ System Design  

```mermaid
flowchart TD
    A[Simulated Order Data (CSV)] -->|Ingest| B[Snowflake Raw Schema]
    B -->|dbt Transformations| C[Snowflake Analytics Schema]
    C -->|Monitor & Flag Delays| D[Airflow DAGs]
    D -->|Trigger SLA| E[Email Alerts for Delayed Orders]
````

---

## ğŸ“‚ Project Structure

```
ecommerce-data-pipeline/
â”‚
â”œâ”€â”€ airflow_dags/                # Airflow DAG definitions
â”‚   â””â”€â”€ ecommerce_pipeline_dag.py
â”‚
â”œâ”€â”€ dbt/                         # dbt project folder
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/             # Standardized raw data
â”‚   â”‚   â”œâ”€â”€ marts/               # Business logic models
â”‚   â”‚   â””â”€â”€ alerts/              # SLA alerting models
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ data/                        # Dummy data
â”‚   â””â”€â”€ orders.csv
â”‚
â”œâ”€â”€ airflow_home/                # Airflow config & logs
â”‚   â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ scripts/                     # Python scripts
â”‚   â”œâ”€â”€ generate_orders.py
â”‚   â””â”€â”€ load_to_snowflake.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ—ƒï¸ E-Commerce ERD

Below is the **Entity Relationship Diagram (ERD)** for the E-Commerce dataset used in this pipeline:

![E-Commerce ERD](./Ecommerce%20ERD.png)

The ERD illustrates the relationships between **customers, orders, products, and deliveries** â€” the same structure that is simulated, ingested, and transformed in this pipeline.

---

## ğŸ“¦ Step 1: Dummy Data Creation

We generate order data with Python (`faker`, `pandas`).

Example schema:

* `order_id`, `customer_id`, `product_id`, `order_date`, `status`, `delivery_date`

```python
from faker import Faker
import pandas as pd
import random

fake = Faker()
orders = []
for i in range(1000):
    orders.append({
        "order_id": i+1,
        "customer_id": fake.random_int(min=1, max=500),
        "product_id": fake.random_int(min=1, max=200),
        "order_date": fake.date_this_year(),
        "status": random.choice(["delivered", "pending", "shipped"]),
        "delivery_date": fake.date_this_year()
    })

df = pd.DataFrame(orders)
df.to_csv("data/orders.csv", index=False)
```

---

## â„ï¸ Step 2: Using Snowflake

* **Database & Schema Setup**

```sql
CREATE OR REPLACE DATABASE ecommerce_db;
CREATE OR REPLACE SCHEMA raw;
CREATE OR REPLACE SCHEMA analytics;

CREATE OR REPLACE TABLE raw.orders (
  order_id INT,
  customer_id INT,
  product_id INT,
  order_date DATE,
  status STRING,
  delivery_date DATE
);
```

* Load CSV into **raw schema** using Snowflake **COPY INTO** or Python Snowflake connector.

---

## ğŸ› ï¸ Step 3: dbt Transformations

### dbt Folder Layers

* **staging/** â†’ Clean, standardize data
* **marts/** â†’ Business-ready metrics (delayed orders, delivery KPIs)
* **alerts/** â†’ SLA alerting queries

### Example: Flagging Delayed Orders

```sql
-- models/marts/delayed_orders.sql
SELECT
    order_id,
    customer_id,
    product_id,
    order_date,
    delivery_date,
    CASE
        WHEN DATEDIFF(day, order_date, delivery_date) > 7 THEN 1
        ELSE 0
    END AS delayed_flag
FROM {{ ref('stg_orders') }}
```

Run transformations:

```bash
dbt run
dbt test
```

---

## ğŸ Step 4: Python Virtual Environment

```bash
# Create and activate venv
python -m venv airflow_venv
airflow_venv\Scripts\activate     # (Windows)
source airflow_venv/bin/activate  # (Linux/Mac)

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸŒ¬ï¸ Step 5: Apache Airflow Orchestration

### Initialize Airflow

```bash
export AIRFLOW_HOME=./airflow_home
airflow db init
```

### Example DAG

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from airflow.utils.dates import days_ago

from scripts.load_to_snowflake import load_data
from scripts.run_dbt import run_dbt_models

with DAG(
    dag_id="ecommerce_pipeline",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
) as dag:

    ingest_task = PythonOperator(
        task_id="load_orders_to_snowflake",
        python_callable=load_data
    )

    transform_task = PythonOperator(
        task_id="run_dbt_transformations",
        python_callable=run_dbt_models
    )

    alert_task = EmailOperator(
        task_id="send_delay_alert",
        to="alerts@company.com",
        subject="ğŸš¨ Delayed Orders Alert",
        html_content="<p>One or more orders exceeded the SLA threshold!</p>"
    )

    ingest_task >> transform_task >> alert_task
```

### Airflow UI

```bash
airflow webserver --port 8080
airflow scheduler
```

ğŸ‘‰ Access at **[http://localhost:8080](http://localhost:8080)**

---

## ğŸ“§ Step 6: SLA Alerts

* SLA thresholds (e.g., >7 days delivery) flagged in **dbt models**
* Alerts configured via **Airflow EmailOperator**

This ensures **real-time monitoring** of operational KPIs.

---

## ğŸ¯ Real-World Relevance

This project helped me **bridge the gap between theory and practice** in Data Engineering by:

*  Building an **end-to-end data pipeline** similar to enterprise workflows
*  Learning to integrate **Snowflake, dbt, and Airflow** into one cohesive system
*  Applying **data modeling principles (ERD, staging, marts, alerts)**
*  Implementing **SLA-driven monitoring**, critical for real-world pipelines
*  Understanding how to **alert stakeholders** automatically when KPIs breach thresholds

By working on this, I learned not just the tools but **how to solve real-world problems in data reliability, monitoring, and automation**.

---

## ğŸ“Œ Future Enhancements

* Real-time ingestion with **Kafka** or streaming connectors
* Extend alerts to **Slack / Teams** integrations
* Deploy Airflow on **MWAA / Cloud Composer** for production-grade scaling

---

## ğŸ¤ Contributing

Contributions are welcome! Open an issue for discussions, improvements, or bug reports.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ“¬ Contact

* ğŸŒ [**Portfolio**](https://kibutujr.vercel.app)
* ğŸ’¼ [**LinkedIn**](https://www.linkedin.com/in/fred-kibutu)
* ğŸ“§ [**Email**](mailto:kibutujr@gmail.com)

---
